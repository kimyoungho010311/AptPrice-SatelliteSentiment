import time
import csv
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

# í¬ë¡¬ ë“œë¼ì´ë²„ ê²½ë¡œ
DRIVER_PATH = "/Users/kim-youngho/Desktop/Sellenium/chromedriver-mac-arm64/chromedriver"

# ì„œìš¸ì‹œ 25ê°œ ìì¹˜êµ¬ ë¦¬ìŠ¤íŠ¸
dvsn_list = [
    'ê°•ë‚¨êµ¬', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ê°•ì„œêµ¬', 'ê´€ì•…êµ¬',
    'ê´‘ì§„êµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ë…¸ì›êµ¬', 'ë„ë´‰êµ¬',
    'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬', 'ë§ˆí¬êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ì„œì´ˆêµ¬',
    'ì„±ë™êµ¬', 'ì„±ë¶êµ¬', 'ì†¡íŒŒêµ¬', 'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬',
    'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ì¤‘ë‘êµ¬'
]

# ë‚ ì§œ ë¬¸ìì—´ ì •ì œ í•¨ìˆ˜
def clean_date(raw_date_str):
    if raw_date_str:
        raw_date_str = raw_date_str.replace('ì…ë ¥', '').strip()
        date_part = raw_date_str.split()[0]
        return date_part.rstrip('.')
    return "ë‚ ì§œ ì—†ìŒ"

# ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜
def clean_content(raw_content):
    if raw_content:
        return "\n".join([line.strip() for line in raw_content.splitlines() if line.strip()])
    return "ë³¸ë¬¸ ì—†ìŒ"

# ì…€ë ˆë‹ˆì›€ ì„¤ì •
options = Options()
options.add_argument("disable-blink-features=AutomationControlled")
options.add_argument("user-agent=Mozilla/5.0")
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option('useAutomationExtension', False)
options.add_experimental_option("detach", True)

# 25ê°œ ìì¹˜êµ¬ ë°˜ë³µ
for dvsn in dvsn_list:
    print(f"\nğŸš© ìì¹˜êµ¬: {dvsn} í¬ë¡¤ë§ ì‹œì‘\n")

    service = Service(executable_path=DRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    driver.get("https://land.naver.com/news/region.naver")
    Select(driver.find_element(By.ID, 'city_no')).select_by_visible_text("ì„œìš¸")
    # time.sleep(1)
    Select(driver.find_element(By.ID, 'dvsn_no')).select_by_visible_text(dvsn)
    # time.sleep(1)

    article_urls = []
    page_limit = 10
    current_page = 1

    while current_page <= page_limit:
        print(f"ğŸ“„ ({dvsn}) í˜„ì¬ í˜ì´ì§€: {current_page}")
        # time.sleep(1)
        news_links = driver.find_elements(By.CSS_SELECTOR, 'dl > dt:nth-child(2) > a')
        urls = [link.get_attribute('href') for link in news_links if link.get_attribute('href')]
        article_urls.extend(urls)

        try:
            next_btn = driver.find_element(By.CSS_SELECTOR, 'a.next')
            next_btn.click()
            current_page += 1
            # time.sleep(2)
        except NoSuchElementException:
            print("âŒ ë” ì´ìƒ ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            break

    driver.quit()

    # ì¤‘ë³µ ì œê±°
    article_urls = list(set(article_urls))
    print(f"âœ… ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ URL ìˆ˜ ({dvsn}): {len(article_urls)}")

    # ê° ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ (ë‚ ì§œ í•„í„°ë§ ì—†ìŒ)
    results = []
    headers = {"User-Agent": "Mozilla/5.0"}

    for idx, url in enumerate(article_urls):
        try:
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')

            title_tag = soup.select_one('h2#title_area')
            title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

            date_tag = soup.select_one('div.media_end_head_info_datestamp_bunch')
            date_raw = date_tag.get_text(strip=True) if date_tag else None
            date_cleaned = clean_date(date_raw)

            content_tag = soup.select_one('article#dic_area')
            content_raw = content_tag.get_text(separator='\n', strip=True) if content_tag else ""
            content_cleaned = clean_content(content_raw)

            results.append({
                "ì œëª©": title,
                "ë‚ ì§œ": date_cleaned,
                "ë³¸ë¬¸": content_cleaned,
                "URL": url
            })

            print(f"ğŸ” [{idx+1}] ìˆ˜ì§‘ ì™„ë£Œ: {title[:30]}...")

        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} / {e}")

    # CSV ì €ì¥
    filename = f"{dvsn}_news.csv"
    with open(filename, "w", newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ì œëª©", "ë‚ ì§œ", "ë³¸ë¬¸", "URL"])
        writer.writeheader()
        for row in results:
            writer.writerow(row)

    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")